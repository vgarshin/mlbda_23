{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Big Data Analysis course\n",
    "## Topic: Advanced Big Data analysis techniques\n",
    "### Part 2. Apache Spark for data processing and ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import udf\n",
    "from pyspark.sql import types \n",
    "from pyspark.sql.types import *\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data processing with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('user:', os.environ['JUPYTERHUB_SERVICE_PREFIX'])\n",
    "\n",
    "def uiWebUrl(self):\n",
    "    from urllib.parse import urlparse\n",
    "    web_url = self._jsc.sc().uiWebUrl().get()\n",
    "    port = urlparse(web_url).port\n",
    "    return '{}proxy/{}/jobs/'.format(os.environ['JUPYTERHUB_SERVICE_PREFIX'], port)\n",
    "\n",
    "SparkContext.uiWebUrl = property(uiWebUrl)\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set('spark.master', 'local[5]')           # max 5 cores available\n",
    "conf.set('spark.driver.memory', '12G')         # max 16 GB available\n",
    "conf.set('spark.driver.maxResultSize', '1G')   # helps sometime\n",
    "# You may play with this settings\n",
    "# but it does not matter for\n",
    "# standalone Spark installation\n",
    "#conf.set('spark.driver.memory', '512m')\n",
    "#conf.set('spark.executor.memory', '2G')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will use train dataset for Microsoft Malware Prediction [competition on Kaggle](https://www.kaggle.com/competitions/microsoft-malware-prediction/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../__DATA/malware_prediction.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.csv(\n",
    "    file_path, \n",
    "    header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can look ugly for many columns\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Load many files at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../__DATA/*.csv'\n",
    "sdf = spark.read.csv(\n",
    "    file_path, \n",
    "    header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Basic operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that we did not call any process computations before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select('Census_GenuineStateName').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select('Wdft_IsGamer', 'Wdft_RegionIdentifier').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select(sdf.Wdft_IsGamer).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Complex pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf \\\n",
    "    .orderBy('Census_InternalPrimaryDiagonalDisplaySizeInInches', ascending=False) \\\n",
    "    .select('MachineIdentifier', 'Census_MDC2FormFactor', 'Census_InternalPrimaryDiagonalDisplaySizeInInches') \\\n",
    "    .limit(10) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf \\\n",
    "    .orderBy('Census_InternalPrimaryDiagonalDisplaySizeInInches', ascending=True) \\\n",
    "    .select('MachineIdentifier', 'Census_MDC2FormFactor', 'Census_InternalPrimaryDiagonalDisplaySizeInInches') \\\n",
    "    .limit(10) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf \\\n",
    "    .limit(1000) \\\n",
    "    .select('Census_InternalPrimaryDiagonalDisplaySizeInInches') \\\n",
    "    .distinct() \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf \\\n",
    "    .orderBy('Census_InternalPrimaryDiagonalDisplaySizeInInches', ascending=False) \\\n",
    "    .select('MachineIdentifier', 'Census_MDC2FormFactor', 'Census_InternalPrimaryDiagonalDisplaySizeInInches') \\\n",
    "    .filter(sdf['Census_MDC2FormFactor'] == 'Notebook') \\\n",
    "    .limit(10) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) is a Spark module for structured data processing. Internally, Spark SQL uses this extra information to perform extra optimizations. There are several ways to interact with Spark SQL including SQL and the Dataset API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.createOrReplaceTempView('malware')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_temp = spark.sql('SELECT * FROM malware LIMIT 10')\n",
    "sdf_temp.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT Census_MDC2FormFactor, Census_InternalPrimaryDiagonalDisplaySizeInInches \n",
    "FROM malware \n",
    "WHERE Census_MDC2FormFactor='Notebook'\n",
    "LIMIT 10\n",
    "'''\n",
    "sdf_temp = spark.sql(query)\n",
    "sdf_temp.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Machine Learning with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype in ['BinaryType', 'BooleanType', 'ByteType', 'DateType', \n",
    "          'DecimalType', 'DoubleType', 'FloatType', 'IntegerType', \n",
    "          'LongType', 'ShortType', 'StringType', 'TimestampType']:\n",
    "    print(f'{dtype}: {getattr(types, dtype)().simpleString()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.limit(1000).describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_cols = [\n",
    "    'IsSxsPassiveMode',\n",
    "    'HasTpm',\n",
    "    'IsProtected',\n",
    "    'Firewall',\n",
    "    'Census_HasOpticalDiskDrive',\n",
    "    'Census_IsPortableOperatingSystem',\n",
    "    'Census_IsSecureBootEnabled',\n",
    "    'Census_IsVirtualDevice',\n",
    "    'Census_IsTouchEnabled',\n",
    "    'Census_IsPenCapable',\n",
    "    'Census_IsAlwaysOnAlwaysConnectedCapable',\n",
    "    'Wdft_IsGamer',\n",
    "    'HasDetections'\n",
    "]\n",
    "for col in dummy_cols:\n",
    "    sdf = sdf.withColumn(col, sdf[col].cast(IntegerType()))\n",
    "    print(col, 'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_bigint = [\n",
    "    'Census_ProcessorCoreCount',\n",
    "    'Census_PrimaryDiskTotalCapacity',\n",
    "    'Census_SystemVolumeTotalCapacity',\n",
    "    'Census_TotalPhysicalRAM',\n",
    "    'Census_InternalPrimaryDisplayResolutionHorizontal',\n",
    "    'Census_InternalPrimaryDisplayResolutionVertical',\n",
    "    'Census_InternalBatteryNumberOfCharges'\n",
    "]\n",
    "for col in cols_to_bigint:\n",
    "    sdf = sdf.withColumn(col, sdf[col].cast(LongType()))\n",
    "    print(col, 'done')\n",
    "\n",
    "cols_to_float = [\n",
    "    'Census_InternalPrimaryDiagonalDisplaySizeInInches'\n",
    "]\n",
    "for col in cols_to_float:\n",
    "    sdf = sdf.withColumn(col, sdf[col].cast(FloatType()))\n",
    "    print(col, 'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "plt.hist(\n",
    "    [\n",
    "        row['Census_TotalPhysicalRAM'] \n",
    "        for row in sdf.limit(10000).select('Census_TotalPhysicalRAM').collect() \n",
    "        if row['Census_TotalPhysicalRAM'] is not None\n",
    "    ], \n",
    "    bins=50\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf \\\n",
    "    .groupBy('HasDetections') \\\n",
    "    .pivot('Wdft_IsGamer') \\\n",
    "    .count() \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf \\\n",
    "    .groupBy('HasDetections') \\\n",
    "    .pivot('Wdft_IsGamer') \\\n",
    "    .mean('Census_TotalPhysicalRAM') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf \\\n",
    "    .groupBy('HasDetections') \\\n",
    "    .pivot('Wdft_IsGamer') \\\n",
    "    .mean('Census_InternalPrimaryDiagonalDisplaySizeInInches') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf \\\n",
    "    .groupBy('HasDetections') \\\n",
    "    .pivot('ProductName') \\\n",
    "    .count() \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf \\\n",
    "    .groupBy('HasDetections') \\\n",
    "    .pivot('ProductName') \\\n",
    "    .count() \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Buliding and training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is [complete guide](https://spark.apache.org/docs/latest/ml-guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use only 10% of total data for training model demo\n",
    "# you may try to use more data for your experiments\n",
    "sdf_train = sdf.sample(fraction=.1)\n",
    "print('training dataset count:', sdf_train.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = 'ProductName'\n",
    "stringIndexer = StringIndexer(inputCol=cat_col, outputCol=cat_col + '_index')\n",
    "sdf_train = stringIndexer.fit(sdf_train).transform(sdf_train)\n",
    "sdf_train.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(\n",
    "    inputCol=stringIndexer.getOutputCol(), \n",
    "    outputCol=cat_col + '_class_vec'\n",
    ")\n",
    "sdf_train = encoder.fit(sdf_train).transform(sdf_train)\n",
    "sdf_train.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can impute missing data\n",
    "# and select strategy for each \n",
    "# type of a column\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=cols_to_bigint, \n",
    "    outputCols=cols_to_bigint\n",
    ")\n",
    "sdf_train = imputer.setStrategy('median').fit(sdf_train).transform(sdf_train)\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=cols_to_float, \n",
    "    outputCols=cols_to_float\n",
    ")\n",
    "sdf_train = imputer.setStrategy('mean').fit(sdf_train).transform(sdf_train)\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=dummy_cols, \n",
    "    outputCols=dummy_cols\n",
    ")\n",
    "sdf_train = imputer.setStrategy('mode').fit(sdf_train).transform(sdf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_model = dummy_cols + cols_to_bigint + cols_to_float + [cat_col + '_class_vec']\n",
    "print('colums to model: ', cols_to_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[VectorAssembler](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html) is needed as a feature transformer that merges multiple columns into a vector column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = sdf_train.select(cols_to_model)\n",
    "vecAssembler = VectorAssembler(\n",
    "    inputCols=[c for c in cols_to_model if c != 'HasDetections'], \n",
    "    outputCol='features'\n",
    ")\n",
    "features_vec = vecAssembler.transform(features)\n",
    "features_vec = features_vec.withColumnRenamed('HasDetections', 'label')\n",
    "features_vec.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector assempled is all we need\n",
    "# to start training with Spark\n",
    "\n",
    "features_data = features_vec.select('label', 'features')\n",
    "features_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your HOME ASSIGNMENT is to implement\n",
    "# `RandomForestClassifier` with the grid search \n",
    "# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.RandomForestClassifier.html\n",
    "# and to improve final score ROC-AUC\n",
    "\n",
    "lr = LogisticRegression(maxIter=20)\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [100., 10., 1., .1, .01]) \\\n",
    "    .addGrid(lr.fitIntercept, [False, True])\\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "cross_val = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=BinaryClassificationEvaluator(),\n",
    "    numFolds=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "feat_train, feat_test = features_data.randomSplit([.8, .2], seed=42)\n",
    "model = cross_val.fit(feat_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(feat_test)\n",
    "predictionLabels = predictions.select('prediction', 'label')\n",
    "metrics = BinaryClassificationMetrics(\n",
    "    predictionLabels.rdd.map(\n",
    "        lambda lines: [float(x) for x in lines]\n",
    "    )\n",
    ")\n",
    "print('ROC AUC: ', metrics.areaUnderROC)\n",
    "print('Area under PR-curve: ', metrics.areaUnderPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
